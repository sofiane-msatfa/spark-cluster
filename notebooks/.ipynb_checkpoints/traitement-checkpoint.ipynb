{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = os.environ.get(\"DEBUG\", \"True\") == \"True\"\n",
    "SCRIPT_ROOT = \"/root/scripts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /root/data/input/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2020_1.zip\n",
      "  inflating: /tmp/tmp.r1H9gk3vGF/On_Time_Marketing_Carrier_On_Time_Performance_(Beginning_January_2018)_2020_1.csv  \n",
      "  inflating: /tmp/tmp.r1H9gk3vGF/readme.html  \n",
      "Archive:  /root/data/input/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2021_1.zip\n",
      "  inflating: /tmp/tmp.P6yLjDYFbA/On_Time_Marketing_Carrier_On_Time_Performance_(Beginning_January_2018)_2021_1.csv  \n",
      "  inflating: /tmp/tmp.P6yLjDYFbA/readme.html  \n",
      "Archive:  /root/data/input/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2022_1.zip\n",
      "  inflating: /tmp/tmp.HWCZiITzXu/On_Time_Marketing_Carrier_On_Time_Performance_(Beginning_January_2018)_2022_1.csv  \n",
      "  inflating: /tmp/tmp.HWCZiITzXu/readme.html  \n",
      "Archive:  /root/data/input/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2023_1.zip\n",
      "  inflating: /tmp/tmp.P0kovWfO9J/On_Time_Marketing_Carrier_On_Time_Performance_(Beginning_January_2018)_2023_1.csv  \n",
      "  inflating: /tmp/tmp.P0kovWfO9J/readme.html  \n",
      "Archive:  /root/data/input/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2024_1.zip\n",
      "  inflating: /tmp/tmp.JMeOdiSZCv/On_Time_Marketing_Carrier_On_Time_Performance_(Beginning_January_2018)_2024_1.csv  \n",
      "  inflating: /tmp/tmp.JMeOdiSZCv/readme.html  \n",
      "Tous les fichiers ZIP ont été traités avec succès\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/root/scripts/unzip.sh'], returncode=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unzip input files\n",
    "subprocess.run([SCRIPT_ROOT + \"/unzip.sh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tous les fichiers CSV ont été traités avec succès\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/root/scripts/upload.sh'], returncode=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload CSV files to HDFS\n",
    "subprocess.run([SCRIPT_ROOT + \"/upload.sh\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBConfig:\n",
    "    HOST = os.environ.get(\"DB_HOST\", \"localhost\")\n",
    "    PORT = os.environ.get(\"DB_PORT\", \"5432\")\n",
    "    DATABASE = os.environ.get(\"DB_NAME\", \"postgres\")\n",
    "    USER = os.environ.get(\"DB_USER\", \"postgres\")\n",
    "    PASSWORD = os.environ.get(\"DB_PASSWORD\", \"password\")\n",
    "\n",
    "    def get_conn(self):\n",
    "        \"\"\"Create a connection to the PostgreSQL database.\"\"\"\n",
    "        return psycopg2.connect(\n",
    "            host=self.HOST,\n",
    "            port=self.PORT,\n",
    "            dbname=self.DATABASE,\n",
    "            user=self.USER,\n",
    "            password=self.PASSWORD,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class File:\n",
    "    def __init__(self, path: str, size: int, owner: str, date: str):\n",
    "        self.path = path\n",
    "        self.extension = path.split(\".\")[-1]\n",
    "        self.size = size\n",
    "        self.name = path.split(\"/\")[-1].split(\".\")[0]\n",
    "        self.owner = owner\n",
    "        self.datetime = datetime.strptime(date, \"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.name} ({self.size} bytes) by {self.owner} on {self.datetime}\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"FlightDataAnalysis\").master(\"spark://hadoop-namenode:7077\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_from_stdout(line: str) -> File:\n",
    "    parts = line.split()\n",
    "    path = parts[-1]\n",
    "    size = int(parts[4])\n",
    "    owner = parts[2]\n",
    "    date = f\"{parts[5]} {parts[6]}\"\n",
    "    return File(path, size, owner, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_hdfs_files(filepath: str, extensions: list[str] = [\"csv\"]) -> list[File]:\n",
    "    try:\n",
    "        lines = (\n",
    "            subprocess.check_output([\"hdfs\", \"dfs\", \"-ls\", filepath], text=True)\n",
    "            .strip()\n",
    "            .split(\"\\n\")\n",
    "        )\n",
    "        files = [\n",
    "            create_file_from_stdout(line) for line in lines if line.startswith(\"-\")\n",
    "        ]\n",
    "        return [file for file in files if file.extension in extensions]\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error listing files in {filepath}: {e}\\n\") if DEBUG else None\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moove_hdfs_files(files: list[File], destination: str) -> None:\n",
    "    for file in files:\n",
    "        try:\n",
    "            subprocess.run([\"hdfs\", \"dfs\", \"-mv\", file.path, destination], check=True)\n",
    "            print(f\"Moved {file} to {destination}\\n\") if DEBUG else None\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error moving {file} to {destination}: {e}\\n\") if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_to_postgres(table_name: str, file: File) -> None:\n",
    "    conn = DBConfig().get_conn()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        hdfs_file_content = subprocess.check_output(\n",
    "            [\"hdfs\", \"dfs\", \"-cat\", file.path], text=True\n",
    "        )\n",
    "        file_like_object = io.StringIO(hdfs_file_content)\n",
    "        cursor.copy_expert(\n",
    "            sql.SQL(\"COPY {} FROM STDIN WITH CSV HEADER\").format(\n",
    "                sql.Identifier(table_name)\n",
    "            ),\n",
    "            file_like_object,\n",
    "        )\n",
    "        conn.commit()  # Valider les changements\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying {file} to {table_name}: {e}\\n\") if DEBUG else None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temp_chunks_dir():\n",
    "    output_dir = f\"/tmp/spark_output/{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    subprocess.run([\"hdfs\", \"dfs\", \"-mkdir\", \"-p\", output_dir], check=True)\n",
    "    print(f\"Created HDFS directory: {output_dir}\\n\") if DEBUG else None\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temp_chunks_files(df, output_dir: str, partition_factor=2) -> list[File]:\n",
    "    num_partitions = cpu_count() * partition_factor  # Dépend du nombre de CPU\n",
    "    (\n",
    "        print(\n",
    "            f\"Writing DataFrame in {num_partitions} partitions to HDFS at {output_dir}\"\n",
    "        )\n",
    "        if DEBUG\n",
    "        else None\n",
    "    )\n",
    "    df.repartition(num_partitions).write.option(\"maxRecordsPerFile\", 100000).mode(\n",
    "        \"overwrite\"\n",
    "    ).csv(output_dir)\n",
    "\n",
    "    return list_hdfs_files(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_temp_chunks_files(output_dir: str) -> None:\n",
    "    try:\n",
    "        subprocess.run([\"hdfs\", \"dfs\", \"-rm\", \"-r\", output_dir], check=True)\n",
    "        print(f\"Cleaned HDFS directory: {output_dir}\") if DEBUG else None\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error cleaning HDFS directory {output_dir}: {e}\") if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_on_time_performance_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS on_time_performance (\n",
    "        FlightDate DATE,\n",
    "        Year INT,\n",
    "        Month INT,\n",
    "        DayofMonth INT,\n",
    "        DayOfWeek INT,\n",
    "        OriginAirportID INT,\n",
    "        DestAirportID INT,\n",
    "        Operating_Airline CHAR(2),\n",
    "        CRSDepTime INT,\n",
    "        DepTime INT,\n",
    "        CRSArrTime INT,\n",
    "        ArrTime INT,\n",
    "        WheelsOff INT,\n",
    "        WheelsOn INT,\n",
    "        Cancelled BOOLEAN,\n",
    "        CRSElapsedTime INT,\n",
    "        ActualElapsedTime INT,\n",
    "        AirTime INT,\n",
    "        Flights INT,\n",
    "        Distance INT,\n",
    "        CarrierDelay INT,\n",
    "        WeatherDelay INT,\n",
    "        NASDelay INT,\n",
    "        SecurityDelay INT,\n",
    "        LateAircraftDelay INT\n",
    "    );\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_on_time_performance_table_exists():\n",
    "    conn = DBConfig().get_conn()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        cursor.execute(create_on_time_performance_table)\n",
    "        conn.commit()\n",
    "        print(\"Table on_time_performance created successfully\") if DEBUG else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating table on_time_performance: {e}\") if DEBUG else None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_transform_on_time_performance_data(df):\n",
    "    \"\"\"\n",
    "    Transforme les données avant l'insertion dans PostgreSQL.\n",
    "    \"\"\"\n",
    "\n",
    "    # Supprimer les espaces blancs dans les noms de colonnes\n",
    "    df = df.select([col(c).alias(c.strip()) for c in df.columns])\n",
    "\n",
    "    # Garder uniquement les colonnes nécessaires\n",
    "    df = df.select(\n",
    "        \"FlightDate\",\n",
    "        \"Year\",\n",
    "        \"Month\",\n",
    "        \"DayofMonth\",\n",
    "        \"DayOfWeek\",\n",
    "        \"OriginAirportID\",\n",
    "        \"DestAirportID\",\n",
    "        \"Operating_Airline\",\n",
    "        \"CRSDepTime\",\n",
    "        \"DepTime\",\n",
    "        \"CRSArrTime\",\n",
    "        \"ArrTime\",\n",
    "        \"WheelsOff\",\n",
    "        \"WheelsOn\",\n",
    "        \"Cancelled\",\n",
    "        \"CRSElapsedTime\",\n",
    "        \"ActualElapsedTime\",\n",
    "        \"AirTime\",\n",
    "        \"Flights\",\n",
    "        \"Distance\",\n",
    "        \"CarrierDelay\",\n",
    "        \"WeatherDelay\",\n",
    "        \"NASDelay\",\n",
    "        \"SecurityDelay\",\n",
    "        \"LateAircraftDelay\",\n",
    "    )\n",
    "\n",
    "    # Supprimer les doublons\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    int_columns = [\"Year\", \"Month\", \"DayofMonth\", \"DayOfWeek\", \"OriginAirportID\", \"DestAirportID\", \n",
    "                   \"Operating_Airline\", \"CRSElapsedTime\", \"ActualElapsedTime\", \"AirTime\", \"Flights\", \n",
    "                   \"Distance\", \"CarrierDelay\", \"WeatherDelay\", \"NASDelay\", \"SecurityDelay\", \"LateAircraftDelay\",\n",
    "                   \"CRSDepTime\", \"DepTime\", \"CRSArrTime\", \"ArrTime\", \"WheelsOff\", \"WheelsOn\"]\n",
    "    \n",
    "    for col_name in int_columns:\n",
    "        df = df.withColumn(col_name, col(col_name).cast(\"int\"))\n",
    "    \n",
    "    # Convertir Cancelled en BOOLEAN\n",
    "    df = df.withColumn(\"Cancelled\", when(col(\"Cancelled\") == 1, True).otherwise(False))\n",
    "    \n",
    "    # Gérer les vols annulés (remplacer les colonnes liées au temps par NULL si annulé)\n",
    "    df = df.withColumn(\"DepTime\", when(col(\"Cancelled\"), None).otherwise(col(\"DepTime\")))\n",
    "    df = df.withColumn(\"ArrTime\", when(col(\"Cancelled\"), None).otherwise(col(\"ArrTime\")))\n",
    "    df = df.withColumn(\"ActualElapsedTime\", when(col(\"Cancelled\"), None).otherwise(col(\"ActualElapsedTime\")))\n",
    "    df = df.withColumn(\"AirTime\", when(col(\"Cancelled\"), None).otherwise(col(\"AirTime\")))\n",
    "    df = df.withColumn(\"CarrierDelay\", when(col(\"Cancelled\"), None).otherwise(col(\"CarrierDelay\")))\n",
    "    df = df.withColumn(\"WeatherDelay\", when(col(\"Cancelled\"), None).otherwise(col(\"WeatherDelay\")))\n",
    "    df = df.withColumn(\"NASDelay\", when(col(\"Cancelled\"), None).otherwise(col(\"NASDelay\")))\n",
    "    df = df.withColumn(\"SecurityDelay\", when(col(\"Cancelled\"), None).otherwise(col(\"SecurityDelay\")))\n",
    "    df = df.withColumn(\"LateAircraftDelay\", when(col(\"Cancelled\"), None).otherwise(col(\"LateAircraftDelay\")))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_on_time_performance_csv(file: File):\n",
    "    print(f\"Processing {file.path}...\\n\") if DEBUG else None\n",
    "    try:\n",
    "        df = spark.read.csv(file.path, header=True, inferSchema=True)\n",
    "        df = clean_and_transform_on_time_performance_data(df)\n",
    "        tmp_dir = create_temp_chunks_dir()\n",
    "        chunks = create_temp_chunks_files(df, tmp_dir)\n",
    "        with Pool(cpu_count()) as pool:\n",
    "            pool.starmap(\n",
    "                copy_to_postgres,\n",
    "                [(\"on_time_performance\", chunk) for chunk in chunks],\n",
    "            )\n",
    "\n",
    "        clean_temp_chunks_files(tmp_dir)\n",
    "        print(f\"{len(chunks)} chunks processed from {file.path}\") if DEBUG else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file.path}: {e}\\n\") if DEBUG else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(path: str, fn) -> None:\n",
    "    files = list_hdfs_files(path)\n",
    "    print(f\"{len(files)} files found in {path}\\n\") if DEBUG else None\n",
    "\n",
    "    for file in files:\n",
    "        fn(file)\n",
    "\n",
    "    moove_hdfs_files(files, \"/completed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table on_time_performance created successfully\n"
     ]
    }
   ],
   "source": [
    "ensure_on_time_performance_table_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 files found in /staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018*\n",
      "\n",
      "Processing /staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2019_1.csv...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created HDFS directory: /tmp/spark_output/20241020205514\n",
      "\n",
      "Writing DataFrame in 32 partitions to HDFS at /tmp/spark_output/20241020205514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/20 20:55:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/spark_output/20241020205514\n",
      "Cleaned HDFS directory: /tmp/spark_output/20241020205514\n",
      "32 chunks processed from /staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2019_1.csv\n",
      "Processing /staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2020_1.csv...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created HDFS directory: /tmp/spark_output/20241020205738\n",
      "\n",
      "Writing DataFrame in 32 partitions to HDFS at /tmp/spark_output/20241020205738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/spark_output/20241020205738\n",
      "Cleaned HDFS directory: /tmp/spark_output/20241020205738\n",
      "32 chunks processed from /staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2020_1.csv\n",
      "Processing /staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2021_1.csv...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created HDFS directory: /tmp/spark_output/20241020205948\n",
      "\n",
      "Writing DataFrame in 32 partitions to HDFS at /tmp/spark_output/20241020205948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/spark_output/20241020205948\n",
      "Cleaned HDFS directory: /tmp/spark_output/20241020205948\n",
      "32 chunks processed from /staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2021_1.csv\n",
      "Processing /staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2022_1.csv...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created HDFS directory: /tmp/spark_output/20241020210200\n",
      "\n",
      "Writing DataFrame in 32 partitions to HDFS at /tmp/spark_output/20241020210200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/spark_output/20241020210200\n",
      "Cleaned HDFS directory: /tmp/spark_output/20241020210200\n",
      "32 chunks processed from /staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2022_1.csv\n",
      "Processing /staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2023_1.csv...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created HDFS directory: /tmp/spark_output/20241020210415\n",
      "\n",
      "Writing DataFrame in 32 partitions to HDFS at /tmp/spark_output/20241020210415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/spark_output/20241020210415\n",
      "Cleaned HDFS directory: /tmp/spark_output/20241020210415\n",
      "32 chunks processed from /staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2023_1.csv\n",
      "Processing /staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2024_1.csv...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created HDFS directory: /tmp/spark_output/20241020210630\n",
      "\n",
      "Writing DataFrame in 32 partitions to HDFS at /tmp/spark_output/20241020210630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/spark_output/20241020210630\n",
      "Cleaned HDFS directory: /tmp/spark_output/20241020210630\n",
      "32 chunks processed from /staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2024_1.csv\n",
      "Moved On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2019_1 (316759084 bytes) by root on 2024-10-20 19:03:00 to /completed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: `/completed': File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error moving On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2020_1 (327859854 bytes) by root on 2024-10-20 20:54:00 to /completed: Command '['hdfs', 'dfs', '-mv', '/staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2020_1.csv', '/completed']' returned non-zero exit status 1.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: `/completed': File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error moving On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2021_1 (188159563 bytes) by root on 2024-10-20 20:54:00 to /completed: Command '['hdfs', 'dfs', '-mv', '/staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2021_1.csv', '/completed']' returned non-zero exit status 1.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: `/completed': File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error moving On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2022_1 (278168409 bytes) by root on 2024-10-20 20:54:00 to /completed: Command '['hdfs', 'dfs', '-mv', '/staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2022_1.csv', '/completed']' returned non-zero exit status 1.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: `/completed': File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error moving On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2023_1 (284746166 bytes) by root on 2024-10-20 20:54:00 to /completed: Command '['hdfs', 'dfs', '-mv', '/staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2023_1.csv', '/completed']' returned non-zero exit status 1.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mv: `/completed': File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error moving On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2024_1 (288685356 bytes) by root on 2024-10-20 20:54:00 to /completed: Command '['hdfs', 'dfs', '-mv', '/staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018_2024_1.csv', '/completed']' returned non-zero exit status 1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "process_data(\n",
    "        path=\"/staging/On_Time_Marketing_Carrier_On_Time_Performance_Beginning_January_2018*\",\n",
    "        fn=process_on_time_performance_csv,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
