services:
  hadoop-namenode:
    container_name: hadoop-namenode
    build:
      context: .
      dockerfile: Dockerfile
    networks:
      - hadoop-spark
    mem_limit: 2g
    cpus: 1
    ports:
      - "50070:50070"
      - "8088:8088"
      - "7077:7077"
      - "8888:8888"
      - "18080:18080"
      - "4040:4040"
      - "10000:10000"
      - "8087:8087"
    hostname: hadoop-namenode
    command: ["sh", "-c", "service ssh start; ./start-cluster.sh; tail -f /dev/null"]
    volumes:
      - ./notebooks:/root/notebooks
      - ./data:/root/data
      - ./database:/root/database
      - ./scripts:/root/scripts

  hadoop-datanode1:
    container_name: hadoop-datanode1
    build:
      context: .
      dockerfile: Dockerfile
    mem_limit: 1.5g
    cpus: 2
    ports:
      - "8081:8081"
    networks:
      - hadoop-spark
    hostname: hadoop-datanode1
    depends_on:
      - hadoop-namenode
    command: ["sh", "-c", "service ssh start; tail -f /dev/null"]

  hadoop-datanode2:
    container_name: hadoop-datanode2
    build:
      context: .
      dockerfile: Dockerfile
    networks:
      - hadoop-spark
    mem_limit: 1.5g
    cpus: 2
    hostname: hadoop-datanode2
    depends_on:
      - hadoop-namenode
    command: ["sh", "-c", "service ssh start; tail -f /dev/null"]

networks:
  hadoop-spark:
    driver: bridge
